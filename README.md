## DPO

[Direct Preference Optimization](https://arxiv.org/abs/2305.18290) is an alternative to RLHF that optimizes the same objective but doesn't require a reward model or online RL. It is much cleaner to implepement than say, PPO (Proximal Policy Optimization). The dataset is a `.pt` file with dicts, where each dict has keys `prompt_chosen_tokens` (tensor, prompt tokens and chosen response tokens), `prompt_rejected_tokens` (prompt tokens and rejected response tokens), `chosen_loss_mask` (the loss mask for `prompt_chosen_tokens`, we only compute loss for the response tokens), and `rejected_loss_mask` (for `prompt_rejected_tokens`).

Dataset is generated by `dataset.py` using Anthropic's HH-RLHF `jsonl` files [here](https://github.com/anthropics/hh-rlhf/tree/master/harmless-base). For non-Ampere GPUs, change `{param, reduce, buffer}_dtype` in `mixed_precision` in `train.py` to something other than `bfloat16`.

Check out my [post](http://okarthikb.github.io/site/blog/dpo.html) for a more in-depth explanation of DPO.

To train, get the `jsonl` files. Then

```
python3 dataset.py --model <path to HF model> --dataset <path to jsonl file>
python3 train.py --nodes <number of nodes> --gpus <gpus per node> --model <path to HF model> ...
```

For `<path to HF model>`, use Eleuther's Pythia or the GPT-2 models.
